"""
IBM Watsonx LLM Integration for KrishiSahay
"""

import os
import requests
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()

class WatsonxLLM:
    def __init__(self):
        """
        Initialize Watsonx LLM with API credentials
        """
        self.api_key = os.getenv("WATSONX_API_KEY")
        self.project_id = os.getenv("WATSONX_PROJECT_ID")
        self.model_id = os.getenv("MODEL_ID", "ibm/granite-3-8b-instruct")
        self.iam_token = None
        self.token_expiry = None
        
        if not self.api_key or not self.project_id:
            print("тЪая╕П  Warning: Watsonx credentials not found in .env file")
            print("Please create a .env file with your credentials for online mode")    
    def get_iam_token(self):
        """
        Get IAM token for Watsonx authentication
        """
        if not self.api_key:
            return None
            
        iam_url = "https://iam.cloud.ibm.com/identity/token"
        iam_data = {
            "apikey": self.api_key,
            "grant_type": "urn:ibm:params:oauth:grant-type:apikey"
        }
        iam_headers = {"Content-Type": "application/x-www-form-urlencoded"}
        
        try:
            response = requests.post(iam_url, data=iam_data, headers=iam_headers)
            response.raise_for_status()
            self.iam_token = response.json()["access_token"]
            return self.iam_token
        except Exception as e:
            print(f"Error getting IAM token: {e}")
            return None
    
    def generate_response(self, query: str, context: str = None, language: str = "hi") -> str:
        """
        Generate response using Watsonx Granite LLM
        """
        if not self.api_key or not self.project_id:
            return self._get_mock_response(query, context)
        
        # Get IAM token if not available
        if not self.iam_token:
            self.get_iam_token()
            if not self.iam_token:
                return "Failed to authenticate with Watsonx. Using offline mode."
        
        # Prepare system prompt based on language
        if language == "hi":
            system_prompt = """рдЖрдк рдХреГрд╖рд┐ рд╕рд╣рд╛рдпрдХ рд╣реИрдВ рдЬреЛ рднрд╛рд░рддреАрдп рдХрд┐рд╕рд╛рдиреЛрдВ рдХреЛ рдХреГрд╖рд┐ рд╕рдВрдмрдВрдзреА рд╕рд▓рд╛рд╣ рджреЗрддреЗ рд╣реИрдВред
рдЖрдкрдХрд╛ рдирд╛рдо KrishiSahay рд╣реИред рдЖрдк рд╣рд┐рдВрджреА рдФрд░ рдЕрдВрдЧреНрд░реЗрдЬреА рдореЗрдВ рдЬрд╡рд╛рдм рджреЗ рд╕рдХрддреЗ рд╣реИрдВред
рд╣рдореЗрд╢рд╛ рд╡рд┐рдирдореНрд░ рдФрд░ рдорджрджрдЧрд╛рд░ рдмрдиреЗрдВред рдЕрдЧрд░ рдХреБрдЫ рдкрддрд╛ рдирд╣реАрдВ рд╣реИ рддреЛ рдИрдорд╛рдирджрд╛рд░реА рд╕реЗ рдХрд╣реЗрдВред
рдХрд┐рд╕рд╛рдиреЛрдВ рдХреЛ "рд╢реНрд░реАрдорд╛рди рдЬреА" рдпрд╛ "рдХрд┐рд╕рд╛рди рднрд╛рдИ" рдХрд╣рдХрд░ рд╕рдВрдмреЛрдзрд┐рдд рдХрд░реЗрдВред"""
        else:
            system_prompt = """You are KrishiSahay, an agricultural assistant for Indian farmers.
You provide practical farming advice in simple Hindi or English.
Be polite, helpful, and honest. Address farmers respectfully."""
        
        # Prepare user message with context
        if context:
            user_message = f"""Here is relevant information from the Kisan Call Centre database:

{context}

Farmer's Question: {query}

Please provide a helpful, accurate response. If the context is relevant, use it. 
If not, use your general knowledge but be honest about it. 
Respond in Hinglish (mix of Hindi and English) for better understanding."""
        else:
            user_message = query
        
        # Watsonx API endpoint
        api_url = "https://eu-de.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29"
        
        headers = {
            "Authorization": f"Bearer {self.iam_token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model_id": self.model_id,
            "project_id": self.project_id,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            "parameters": {
                "decoding_method": "greedy",
                "max_new_tokens": 500,
                "temperature": 0.3,
                "top_p": 0.9
            }
        }
        
        try:
            response = requests.post(api_url, headers=headers, json=payload)
            response.raise_for_status()
            
            result = response.json()
            return result.get('choices', [{}])[0].get('message', {}).get('content', 'No response generated')
            
        except Exception as e:
            print(f"Error calling Watsonx API: {e}")
            return self._get_mock_response(query, context)
    
    def _get_mock_response(self, query: str, context: str = None) -> str:
        """
        Return a mock response for testing when Watsonx is not available
        """
        if "рдХреАрдЯ" in query or "рдХреАрдбрд╝рд╛" in query or "aphid" in query.lower():
            return """рд╢реНрд░реАрдорд╛рди рдЬреА, рдХреАрдЯ рдирд┐рдпрдВрддреНрд░рдг рдХреЗ рд▓рд┐рдП:

1. рдиреАрдо рддреЗрд▓ (2%) рдХрд╛ рдЫрд┐рдбрд╝рдХрд╛рд╡ рдХрд░реЗрдВ - 20 рдорд┐рд▓реА рдиреАрдо рддреЗрд▓ рдкреНрд░рддрд┐ рд▓реАрдЯрд░ рдкрд╛рдиреА
2. рдЗрдорд┐рдбрд╛рдХреНрд▓реЛрдлрд┐рдб 17.8 SL 100 ml рдкреНрд░рддрд┐ рдПрдХрдбрд╝ 200 рд▓реАрдЯрд░ рдкрд╛рдиреА рдореЗрдВ рдорд┐рд▓рд╛рдХрд░ рдЫрд┐рдбрд╝рдХрд╛рд╡ рдХрд░реЗрдВ
3. рдЫрд┐рдбрд╝рдХрд╛рд╡ рд╕реБрдмрд╣ рдпрд╛ рд╢рд╛рдо рдХреЗ рд╕рдордп рдХрд░реЗрдВ

рдзрдиреНрдпрд╡рд╛рдж!"""
        
        elif "рдореВрдВрдЧ" in query or "moong" in query.lower():
            return """рд╢реНрд░реАрдорд╛рди рдЬреА, рдореВрдВрдЧ рдХреА рдмреБрд╡рд╛рдИ рдХрд╛ рд╕рд╣реА рд╕рдордп:

тЬЕ рдЧреНрд░реАрд╖реНрдордХрд╛рд▓реАрди рдореВрдВрдЧ: 10 рдорд╛рд░реНрдЪ рд╕реЗ 10 рдЕрдкреНрд░реИрд▓
тЬЕ рдЦрд░реАрдл рдореВрдВрдЧ: рдЬреВрди-рдЬреБрд▓рд╛рдИ
тЬЕ рдЙрдиреНрдирдд рдХрд┐рд╕реНрдореЗрдВ: рдкрдВрдд рдореВрдВрдЧ-5, рдПрд╕рдПрдордПрд▓-668

рдмреБрд╡рд╛рдИ рд╕реЗ рдкрд╣рд▓реЗ рдмреАрдЬ рдХреЛ рдЙрдкрдЪрд╛рд░рд┐рдд рдХрд░рдирд╛ рди рднреВрд▓реЗрдВред"""
        
        else:
            return f"""рд╢реНрд░реАрдорд╛рди рдЬреА, рдЖрдкрдХреЗ рд╕рд╡рд╛рд▓ рдХреЗ рд▓рд┐рдП рдзрдиреНрдпрд╡рд╛рджред

рдЖрдкрдиреЗ рдкреВрдЫрд╛: {query}

рдЗрд╕ рдмрд╛рд░реЗ рдореЗрдВ рдЕрдзрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА рдХреЗ рд▓рд┐рдП рдХреГрдкрдпрд╛:
1. рдЕрдкрдиреЗ рдирдЬрджреАрдХреА рдХреГрд╖рд┐ рд╡рд┐рдЬреНрдЮрд╛рди рдХреЗрдВрджреНрд░ рд╕реЗ рд╕рдВрдкрд░реНрдХ рдХрд░реЗрдВ
2. рдХрд┐рд╕рд╛рди рдХреЙрд▓ рд╕реЗрдВрдЯрд░ 1800-180-1551 рдкрд░ рдХреЙрд▓ рдХрд░реЗрдВ

рдзрдиреНрдпрд╡рд╛рдж! - рдЖрдкрдХреЗ KrishiSahay"""
    
    def generate_with_retrieval(self, query: str, retrieved_results: list) -> str:
        """
        Generate response using retrieved context
        """
        # Format context
        context = "Kisan Call Centre Database:\n\n"
        for i, result in enumerate(retrieved_results, 1):
            meta = result['metadata']
            context += f"{i}. рдкреНрд░рд╢реНрди: {meta['question']}\n"
            context += f"   рдЙрддреНрддрд░: {meta['answer']}\n"
            context += f"   (рдлрд╕рд▓: {meta['crop']})\n\n"
        
        return self.generate_response(query, context)
# Test the LLM
if __name__ == "__main__":
    print("=" * 60)
    print("ЁЯдЦ TESTING WATSONX LLM (MOCK MODE)")
    print("=" * 60)
    
    llm = WatsonxLLM()
    
    # Test queries
    test_queries = [
        "рд╕рд░рд╕реЛрдВ рдореЗрдВ рдХреАрдЯ рдХреИрд╕реЗ рдирд┐рдпрдВрддреНрд░рд┐рдд рдХрд░реЗрдВ?",
        "рдореВрдВрдЧ рдмреЛрдиреЗ рдХрд╛ рд╕рд╣реА рд╕рдордп рдХреНрдпрд╛ рд╣реИ?",
        "рдЧреЗрд╣реВрдВ рдореЗрдВ рдЦрд╛рдж рдХрдм рдбрд╛рд▓реЗрдВ?",
        "PM рдХрд┐рд╕рд╛рди рдпреЛрдЬрдирд╛ рдХреЗ рд▓рд┐рдП рдЖрд╡реЗрджрди рдХреИрд╕реЗ рдХрд░реЗрдВ?"
    ]
    
    for query in test_queries:
        print(f"\nЁЯУЭ Query: {query}")
        print("-" * 50)
        
        response = llm.generate_response(query)
        print(f"ЁЯдЦ Response:\n{response}")
        print("-" * 50)
    
    # Test with retrieved context
    print("\nЁЯУЪ Testing with retrieved context:")
    print("-" * 50)
    
    mock_results = [
        {
            'metadata': {
                'question': 'рдореВрдВрдЧ рдХреА рдмреБрд╡рд╛рдИ рдХрд╛ рд╕рд╣реА рд╕рдордп рдХреНрдпрд╛ рд╣реИ?',
                'answer': 'рдореВрдВрдЧ рдХреА рдмреБрд╡рд╛рдИ рдХрд╛ рдЙрдкрдпреБрдХреНрдд рд╕рдордп 10 рдорд╛рд░реНрдЪ рд╕реЗ 10 рдЕрдкреНрд░реИрд▓ рддрдХ рд╣реИред',
                'crop': 'рдореВрдВрдЧ'
            }
        }
    ]
    
    response = llm.generate_with_retrieval("рдореВрдВрдЧ рдХрдм рдмреЛрдПрдВ?", mock_results)
    print(f"ЁЯдЦ Response with context:\n{response}")
    
    print("\n" + "=" * 60)
    print("тЬЕ Test complete! Watsonx LLM (Mock Mode) is working!")
    print("=" * 60)
